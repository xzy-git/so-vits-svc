{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1hj9CSF0Rxfvmo-VPcqYTdW6qKEqgL6oB","timestamp":1674307861721},{"file_id":"1-OxJUdnc1Vc6324N-UH-nRsrUJSaU9hK","timestamp":1673246286754},{"file_id":"1Q2IxQLtuH2KUM1QT-Z51Gd65F6eXKvqV","timestamp":1672858960751},{"file_id":"1rCUOOVG7-XQlVZuWRAj5IpGrMM8t07pE","timestamp":1671865645484},{"file_id":"1Ul5SmzWiSHBj0MaKA0B682C-RZKOycwF","timestamp":1670483515921}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# 推理"],"metadata":{"id":"TrsHTUilDoxQ"}},{"cell_type":"code","source":["# 查看显卡\n","!nvidia-smi"],"metadata":{"id":"0gQcIZ8RsOkn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674715328865,"user_tz":-480,"elapsed":1255,"user":{"displayName":"zy x","userId":"01000903033234232137"}},"outputId":"d8badd4f-5c84-4add-a943-e260f8fdabc5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jan 26 06:42:06 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   71C    P0    32W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674715337613,"user_tz":-480,"elapsed":3700,"user":{"displayName":"zy x","userId":"01000903033234232137"}},"outputId":"e9dc19cd-7403-4ad9-b00c-ea8302b6c38b","cellView":"form","id":"uS6YNc7JqCw1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'so-vits-svc'...\n","remote: Enumerating objects: 390, done.\u001b[K\n","remote: Counting objects: 100% (112/112), done.\u001b[K\n","remote: Compressing objects: 100% (65/65), done.\u001b[K\n","remote: Total 390 (delta 74), reused 59 (delta 47), pack-reused 278\u001b[K\n","Receiving objects: 100% (390/390), 28.08 MiB | 15.52 MiB/s, done.\n","Resolving deltas: 100% (158/158), done.\n"]}],"source":["#@title 克隆的github仓库\n","#@markdown ##选择要克隆的github仓库分支\n","Clone = \"48k\" #@param [\"32k\",\"48k\"]\n","if Clone == \"32k\":\n","  !git clone https://github.com/xzy-git/so-vits-svc -b new32k\n","elif Clone == \"48k\":\n","  !git clone https://github.com/xzy-git/so-vits-svc -b new48k"]},{"cell_type":"code","source":["#@title 安装依赖\n","%cd /content/so-vits-svc\n","!pip install pyworld praat-parselmouth\n"],"metadata":{"id":"zXBLkXxL4T1O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674715377393,"user_tz":-480,"elapsed":31066,"user":{"displayName":"zy x","userId":"01000903033234232137"}},"outputId":"9071de17-5dc9-463e-964f-ff86cc737f69","cellView":"form"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/so-vits-svc\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyworld\n","  Downloading pyworld-0.3.2.tar.gz (214 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting praat-parselmouth\n","  Downloading praat_parselmouth-0.4.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (10.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pyworld) (1.21.6)\n","Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from pyworld) (0.29.33)\n","Building wheels for collected packages: pyworld\n","  Building wheel for pyworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyworld: filename=pyworld-0.3.2-cp38-cp38-linux_x86_64.whl size=919633 sha256=e50fee611833481727f5a235be0612809f8799ae3b56c405cece723ca8eb288a\n","  Stored in directory: /root/.cache/pip/wheels/b7/b1/d2/8c78d691f7d5b0bb4ba9993926db209429c92686476837627f\n","Successfully built pyworld\n","Installing collected packages: pyworld, praat-parselmouth\n","Successfully installed praat-parselmouth-0.4.3 pyworld-0.3.2\n"]}]},{"cell_type":"code","source":["#@title 下载必要模型文件\n","!wget -P hubert/ https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt"],"metadata":{"id":"pCqf3W0d6ify","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1674715487103,"user_tz":-480,"elapsed":23719,"user":{"displayName":"zy x","userId":"01000903033234232137"}},"outputId":"42f4e398-d396-4d87-c730-893ff6387955"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-01-26 06:44:21--  https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt\n","Resolving github.com (github.com)... 140.82.121.4\n","Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/417578841/6eaffd96-4bcb-4978-ac67-80857af26838?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230126%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230126T064421Z&X-Amz-Expires=300&X-Amz-Signature=48c8f82e76be94b024df4ab00c8a539deafa3f5f5f4430851c634c17c955ffa8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=417578841&response-content-disposition=attachment%3B%20filename%3Dhubert-soft-0d54a1f4.pt&response-content-type=application%2Foctet-stream [following]\n","--2023-01-26 06:44:21--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/417578841/6eaffd96-4bcb-4978-ac67-80857af26838?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230126%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230126T064421Z&X-Amz-Expires=300&X-Amz-Signature=48c8f82e76be94b024df4ab00c8a539deafa3f5f5f4430851c634c17c955ffa8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=417578841&response-content-disposition=attachment%3B%20filename%3Dhubert-soft-0d54a1f4.pt&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 378435957 (361M) [application/octet-stream]\n","Saving to: ‘hubert/hubert-soft-0d54a1f4.pt’\n","\n","hubert-soft-0d54a1f 100%[===================>] 360.90M  10.9MB/s    in 23s     \n","\n","2023-01-26 06:44:44 (15.8 MB/s) - ‘hubert/hubert-soft-0d54a1f4.pt’ saved [378435957/378435957]\n","\n"]}]},{"cell_type":"code","source":["#@title 加载Google云端硬盘\n","#@markdown 加载Google云端硬盘\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"wmUkpUmfn_Hs","executionInfo":{"status":"ok","timestamp":1674715511351,"user_tz":-480,"elapsed":21335,"user":{"displayName":"zy x","userId":"01000903033234232137"}},"outputId":"8d5bd68c-1af1-410a-dcca-48c1b9927d6f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#@title 将模型导出为onnx供本地推理用\n","!pip install onnx\n","!pip install onnxsim\n","!pip install onnxruntime\n","!pip install onnxoptimizer\n","\n","#@markdown **是否导出onnx格式的hubert**\n","export_hubert = False #@param {type:\"boolean\"}\n","#@markdown **是否导出onnx格式的模型**\n","export_model = True #@param {type:\"boolean\"}\n","#@markdown **模型路径**\n","model_path = \"/content/drive/MyDrive/48k\" #@param {type:\"string\"}\n","#@markdown **pth模型文件名，如有多个，文件名以逗号分隔**\n","model_files_input = \"G_5000,G_6000,G_7000,G_8000,G_9000\" #@param {type:\"string\"}\n","model_files = model_files_input.split(',') \n","\n","for i in range(len(model_files)):\n","  !python onnx_export/onnx_export.py --export_hubert \"{export_hubert}\" --export_model \"{export_model}\" --model_path \"{model_path}\" --model_file \"{model_files[i]}\""],"metadata":{"id":"KiNCWprSPlKH","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1674715812205,"user_tz":-480,"elapsed":291886,"user":{"displayName":"zy x","userId":"01000903033234232137"}},"outputId":"20ce3ece-715c-4ba5-d34f-434d27b99db9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting onnx\n","  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx) (4.4.0)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx) (1.21.6)\n","Collecting protobuf<4,>=3.20.2\n","  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: protobuf, onnx\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.19.6\n","    Uninstalling protobuf-3.19.6:\n","      Successfully uninstalled protobuf-3.19.6\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed onnx-1.13.0 protobuf-3.20.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting onnxsim\n","  Downloading onnxsim-0.4.13-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: onnx in /usr/local/lib/python3.8/dist-packages (from onnxsim) (1.13.0)\n","Collecting rich\n","  Downloading rich-13.2.0-py3-none-any.whl (238 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxsim) (1.21.6)\n","Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxsim) (3.20.3)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxsim) (4.4.0)\n","Collecting markdown-it-py<3.0.0,>=2.1.0\n","  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich->onnxsim) (2.6.1)\n","Collecting mdurl~=0.1\n","  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n","Installing collected packages: mdurl, markdown-it-py, rich, onnxsim\n","Successfully installed markdown-it-py-2.1.0 mdurl-0.1.2 onnxsim-0.4.13 rich-13.2.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting onnxruntime\n","  Downloading onnxruntime-1.13.1-cp38-cp38-manylinux_2_27_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.7.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (3.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (21.3)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.21.6)\n","Collecting coloredlogs\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.12)\n","Collecting humanfriendly>=9.1\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->onnxruntime) (3.0.9)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime) (1.2.1)\n","Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.13.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting onnxoptimizer\n","  Downloading onnxoptimizer-0.3.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (606 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.4/606.4 KB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: onnx in /usr/local/lib/python3.8/dist-packages (from onnxoptimizer) (1.13.0)\n","Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxoptimizer) (3.20.3)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxoptimizer) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxoptimizer) (4.4.0)\n","Installing collected packages: onnxoptimizer\n","Successfully installed onnxoptimizer-0.3.6\n","False\n","True\n","/content/drive/MyDrive/48k\n","G_5000\n","INFO:root:Loaded checkpoint '/content/drive/MyDrive/48k/G_5000.pth' (iteration 13)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input hidden_unit\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input pitch\n","  warnings.warn(\n","/content/so-vits-svc/onnx_export/utils.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert f0_coarse.max() <= 255 and f0_coarse.min() >= 1, (f0_coarse.max(), f0_coarse.min())\n","/content/so-vits-svc/onnx_export/attentions.py:157: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n","/content/so-vits-svc/onnx_export/attentions.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  pad_length = max(length - (self.window_size + 1), 0)\n","/content/so-vits-svc/onnx_export/attentions.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  slice_start_position = max((self.window_size + 1) - length, 0)\n","/content/so-vits-svc/onnx_export/attentions.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if pad_length > 0:\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","False\n","True\n","/content/drive/MyDrive/48k\n","G_6000\n","INFO:root:Loaded checkpoint '/content/drive/MyDrive/48k/G_6000.pth' (iteration 15)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input hidden_unit\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input pitch\n","  warnings.warn(\n","/content/so-vits-svc/onnx_export/utils.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert f0_coarse.max() <= 255 and f0_coarse.min() >= 1, (f0_coarse.max(), f0_coarse.min())\n","/content/so-vits-svc/onnx_export/attentions.py:157: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n","/content/so-vits-svc/onnx_export/attentions.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  pad_length = max(length - (self.window_size + 1), 0)\n","/content/so-vits-svc/onnx_export/attentions.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  slice_start_position = max((self.window_size + 1) - length, 0)\n","/content/so-vits-svc/onnx_export/attentions.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if pad_length > 0:\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","False\n","True\n","/content/drive/MyDrive/48k\n","G_7000\n","INFO:root:Loaded checkpoint '/content/drive/MyDrive/48k/G_7000.pth' (iteration 18)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input hidden_unit\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input pitch\n","  warnings.warn(\n","/content/so-vits-svc/onnx_export/utils.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert f0_coarse.max() <= 255 and f0_coarse.min() >= 1, (f0_coarse.max(), f0_coarse.min())\n","/content/so-vits-svc/onnx_export/attentions.py:157: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n","/content/so-vits-svc/onnx_export/attentions.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  pad_length = max(length - (self.window_size + 1), 0)\n","/content/so-vits-svc/onnx_export/attentions.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  slice_start_position = max((self.window_size + 1) - length, 0)\n","/content/so-vits-svc/onnx_export/attentions.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if pad_length > 0:\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","False\n","True\n","/content/drive/MyDrive/48k\n","G_8000\n","INFO:root:Loaded checkpoint '/content/drive/MyDrive/48k/G_8000.pth' (iteration 20)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input hidden_unit\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input pitch\n","  warnings.warn(\n","/content/so-vits-svc/onnx_export/utils.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert f0_coarse.max() <= 255 and f0_coarse.min() >= 1, (f0_coarse.max(), f0_coarse.min())\n","/content/so-vits-svc/onnx_export/attentions.py:157: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n","/content/so-vits-svc/onnx_export/attentions.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  pad_length = max(length - (self.window_size + 1), 0)\n","/content/so-vits-svc/onnx_export/attentions.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  slice_start_position = max((self.window_size + 1) - length, 0)\n","/content/so-vits-svc/onnx_export/attentions.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if pad_length > 0:\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","False\n","True\n","/content/drive/MyDrive/48k\n","G_9000\n","INFO:root:Loaded checkpoint '/content/drive/MyDrive/48k/G_9000.pth' (iteration 23)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input hidden_unit\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input pitch\n","  warnings.warn(\n","/content/so-vits-svc/onnx_export/utils.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert f0_coarse.max() <= 255 and f0_coarse.min() >= 1, (f0_coarse.max(), f0_coarse.min())\n","/content/so-vits-svc/onnx_export/attentions.py:157: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n","/content/so-vits-svc/onnx_export/attentions.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  pad_length = max(length - (self.window_size + 1), 0)\n","/content/so-vits-svc/onnx_export/attentions.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  slice_start_position = max((self.window_size + 1) - length, 0)\n","/content/so-vits-svc/onnx_export/attentions.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if pad_length > 0:\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n","  _C._jit_pass_onnx_graph_shape_type_inference(\n"]}]},{"cell_type":"code","source":["#@title 如需在线推理，则加载模型\n","\n","!pip install scikit-maad\n","\n","import io\n","import logging\n","import time\n","from pathlib import Path\n","\n","import librosa\n","import numpy as np\n","import soundfile\n","import IPython.display as ipd\n","from inference import infer_tool\n","from inference import slicer\n","from inference.infer_tool import Svc\n","\n","logging.getLogger('numba').setLevel(logging.WARNING)\n","chunks_dict = infer_tool.read_temp(\"inference/chunks_temp.json\")\n","\n","#@markdown 模型文件夹位置\n","cloud_model_folder = \"/content/drive/MyDrive/48k\" #@param {type:\"string\"}\n","\n","#@markdown 模型文件名\n","cloud_model_name = \"G_5000.pth\" #@param {type:\"string\"}\n","\n","cloud_model_path = cloud_model_folder + \"/\" + cloud_model_name\n","\n","#@markdown 配置文件位置\n","cloud_config_path = \"/content/drive/MyDrive/48k/config.json\" #@param {type:\"string\"}\n","\n","!cp -f {cloud_config_path} /content/so-vits-svc/configs\n","!cp {cloud_model_path} /content/so-vits-svc/logs/48k\n","\n","model_path = \"/content/so-vits-svc/logs/48k/\" + cloud_model_name\n","config_path = \"/content/so-vits-svc/configs/config.json\"\n","svc_model = Svc(model_path, config_path)\n","infer_tool.mkdir([\"raw\", \"results\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"yUlw6PA_xiG8","executionInfo":{"status":"ok","timestamp":1674715905825,"user_tz":-480,"elapsed":19442,"user":{"displayName":"zy x","userId":"01000903033234232137"}},"outputId":"45b22c0d-cebf-406c-8711-3edf61f24a8b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-maad\n","  Downloading scikit_maad-1.3.12-py3-none-any.whl (142 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.4/142.4 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from scikit-maad) (1.7.3)\n","Requirement already satisfied: scikit-image>=0.17 in /usr/local/lib/python3.8/dist-packages (from scikit-maad) (0.18.3)\n","Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-maad) (1.3.5)\n","Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.8/dist-packages (from scikit-maad) (1.21.6)\n","Requirement already satisfied: resampy>=0.2 in /usr/local/lib/python3.8/dist-packages (from scikit-maad) (0.4.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1->scikit-maad) (2022.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1->scikit-maad) (2.8.2)\n","Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.8/dist-packages (from resampy>=0.2->scikit-maad) (0.56.4)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.17->scikit-maad) (2022.10.10)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.17->scikit-maad) (1.4.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.17->scikit-maad) (2.9.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.17->scikit-maad) (3.0)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.17->scikit-maad) (7.1.2)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.17->scikit-maad) (3.2.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.17->scikit-maad) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.17->scikit-maad) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.17->scikit-maad) (1.4.4)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.53->resampy>=0.2->scikit-maad) (6.0.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.53->resampy>=0.2->scikit-maad) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.53->resampy>=0.2->scikit-maad) (57.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1->scikit-maad) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.53->resampy>=0.2->scikit-maad) (3.11.0)\n","Installing collected packages: scikit-maad\n","Successfully installed scikit-maad-1.3.12\n"]}]},{"cell_type":"code","source":["#@title 在线推理，生成的音频文件在results文件夹内\n","\n","#@markdown 需先将原音频放在raw文件夹下，支持多个wav文件。如有多个，文件名以逗号分隔\n","clean_names_input=\"WHITE_ALBUM_1998_vocals,02770\" #@param {type:\"string\"}\n","clean_names = clean_names_input.split(',') \n","\n","#@markdown 音高调整，支持正负（半音）。如有多个，以逗号分隔\n","trans_input = \"-3,0\" #@param {type:\"string\"}\n","trans = trans_input.split(',')\n","for i in range(len(trans)):\n","  trans[i] = int(trans[i])\n","\n","#@markdown 每次同时合成多语者音色，如有多个，以逗号分隔\n","spk_list_input = \"Altoria\" #@param {type:\"string\"}\n","spk_list = spk_list_input.split(',')\n","\n","#@markdown 默认-40，嘈杂的音频可以-30，干声保留呼吸可以-50\n","slice_db = -40 #@param {type:\"number\"}\n","\n","#@markdown 音频输出格式\n","wav_format = 'flac' #@param {type:\"string\"}\n","\n","infer_tool.fill_a_to_b(trans, clean_names)\n","for clean_name, tran in zip(clean_names, trans):\n","    raw_audio_path = f\"raw/{clean_name}\"\n","    if \".\" not in raw_audio_path:\n","        raw_audio_path += \".wav\"\n","    infer_tool.format_wav(raw_audio_path)\n","    wav_path = Path(raw_audio_path).with_suffix('.wav')\n","    audio, sr = librosa.load(wav_path, mono=True, sr=None)\n","    wav_hash = infer_tool.get_md5(audio)\n","    if wav_hash in chunks_dict.keys():\n","        print(\"load chunks from temp\")\n","        chunks = chunks_dict[wav_hash][\"chunks\"]\n","    else:\n","        chunks = slicer.cut(wav_path, db_thresh=slice_db)\n","    print(chunks)\n","    chunks_dict[wav_hash] = {\"chunks\": chunks, \"time\": int(time.time())}\n","    infer_tool.write_temp(\"inference/chunks_temp.json\", chunks_dict)\n","    audio_data, audio_sr = slicer.chunks2audio(wav_path, chunks)\n","\n","    for spk in spk_list:\n","        audio = []\n","        for (slice_tag, data) in audio_data:\n","            print(f'#=====segment start, {round(len(data) / audio_sr, 3)}s======')\n","            length = int(np.ceil(len(data) / audio_sr * svc_model.target_sample))\n","            raw_path = io.BytesIO()\n","            soundfile.write(raw_path, data, audio_sr, format=\"wav\")\n","            raw_path.seek(0)\n","            if slice_tag:\n","                print('jump empty segment')\n","                _audio = np.zeros(length)\n","            else:\n","                out_audio, out_sr = svc_model.infer(spk, tran, raw_path)\n","                _audio = out_audio.cpu().numpy()\n","            audio.extend(list(_audio))\n","\n","        res_path = f'./results/{clean_name}_{tran}key_{spk}.{wav_format}'\n","        soundfile.write(res_path, audio, svc_model.target_sample, format=wav_format)"],"metadata":{"id":"jxm7D--GTxgI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674716001249,"user_tz":-480,"elapsed":16791,"user":{"displayName":"zy x","userId":"01000903033234232137"}},"outputId":"5c61b20b-1446-414e-ace0-616439adac88","cellView":"form"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{'0': {'slice': False, 'split_time': '0,163170'}, '1': {'slice': True, 'split_time': '163170,163170'}, '2': {'slice': False, 'split_time': '163170,388080'}, '3': {'slice': True, 'split_time': '388080,388080'}, '4': {'slice': False, 'split_time': '388080,652680'}, '5': {'slice': True, 'split_time': '652680,652680'}, '6': {'slice': False, 'split_time': '652680,960498'}, '7': {'slice': True, 'split_time': '960498,960498'}, '8': {'slice': False, 'split_time': '960498,1181880'}, '9': {'slice': True, 'split_time': '1181880,1181880'}, '10': {'slice': False, 'split_time': '1181880,1408554'}, '11': {'slice': True, 'split_time': '1408554,1408554'}, '12': {'slice': False, 'split_time': '1408554,1704024'}, '13': {'slice': True, 'split_time': '1704024,1704024'}, '14': {'slice': False, 'split_time': '1704024,1924524'}, '15': {'slice': True, 'split_time': '1924524,1924524'}, '16': {'slice': False, 'split_time': '1924524,2148552'}, '17': {'slice': True, 'split_time': '2148552,2148552'}, '18': {'slice': False, 'split_time': '2148552,3206952'}, '19': {'slice': True, 'split_time': '3206952,3206952'}, '20': {'slice': False, 'split_time': '3206952,3722922'}, '21': {'slice': True, 'split_time': '3722922,3722922'}, '22': {'slice': False, 'split_time': '3722922,4252122'}, '23': {'slice': True, 'split_time': '4252122,4252122'}, '24': {'slice': False, 'split_time': '4252122,4569642'}, '25': {'slice': True, 'split_time': '4569642,4569642'}, '26': {'slice': False, 'split_time': '4569642,4979772'}, '27': {'slice': True, 'split_time': '4979772,4979772'}, '28': {'slice': False, 'split_time': '4979772,5178222'}, '29': {'slice': True, 'split_time': '5178222,5178222'}, '30': {'slice': False, 'split_time': '5178222,5547780'}, '31': {'slice': True, 'split_time': '5547780,5814191'}}\n","#=====segment start, 3.7s======\n","hubert use time:1.379185438156128\n","vits use time:0.5454626083374023\n","#=====segment start, 5.1s======\n","hubert use time:0.008101940155029297\n","vits use time:0.11954474449157715\n","#=====segment start, 6.0s======\n","hubert use time:0.008344411849975586\n","vits use time:0.1354835033416748\n","#=====segment start, 6.98s======\n","hubert use time:0.008103370666503906\n","vits use time:0.14955592155456543\n","#=====segment start, 5.02s======\n","hubert use time:0.007824897766113281\n","vits use time:0.11627745628356934\n","#=====segment start, 5.14s======\n","hubert use time:0.009253263473510742\n","vits use time:0.11339592933654785\n","#=====segment start, 6.7s======\n","hubert use time:0.008018016815185547\n","vits use time:0.1454765796661377\n","#=====segment start, 5.0s======\n","hubert use time:0.007483243942260742\n","vits use time:0.1138768196105957\n","#=====segment start, 5.08s======\n","hubert use time:0.00727081298828125\n","vits use time:0.11500287055969238\n","#=====segment start, 24.0s======\n","hubert use time:0.012446165084838867\n","vits use time:0.6808822154998779\n","#=====segment start, 11.7s======\n","hubert use time:0.00766301155090332\n","vits use time:0.26468372344970703\n","#=====segment start, 12.0s======\n","hubert use time:0.00801396369934082\n","vits use time:0.271465539932251\n","#=====segment start, 7.2s======\n","hubert use time:0.009601593017578125\n","vits use time:0.1552426815032959\n","#=====segment start, 9.3s======\n","hubert use time:0.00762176513671875\n","vits use time:0.19338560104370117\n","#=====segment start, 4.5s======\n","hubert use time:0.00728154182434082\n","vits use time:0.10541677474975586\n","#=====segment start, 8.38s======\n","hubert use time:0.007960081100463867\n","vits use time:0.179443359375\n","#=====segment start, 6.041s======\n","jump empty segment\n","{'0': {'slice': False, 'split_time': '0,117747'}, '1': {'slice': True, 'split_time': '117747,117747'}, '2': {'slice': False, 'split_time': '117747,220500'}, '3': {'slice': True, 'split_time': '220500,225440'}}\n","#=====segment start, 5.34s======\n","hubert use time:0.007203340530395508\n","vits use time:0.1161489486694336\n","#=====segment start, 4.66s======\n","hubert use time:0.007022857666015625\n","vits use time:0.10785818099975586\n","#=====segment start, 0.224s======\n","jump empty segment\n"]}]}]}